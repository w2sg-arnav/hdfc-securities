{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1kp12elAtOrNMzqGM-036AhLcm0WfiD77","timestamp":1736413115284}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install PyMuPDF\n","!pip install python-doctr\n","!pip install tf2onnx\n","!pip install ultralytics\n","!pip install pandas\n","!pip install scikit-learn\n","!pip install google-generativeai\n","!pip install pdf2image\n","!pip install pinecone-client\n","!pip install tf-keras\n","!pip install together\n","!pip install -U scikit-learn\n","from IPython.display import clear_output\n","clear_output()"],"metadata":{"id":"bh_gQ1TGRFQo","executionInfo":{"status":"ok","timestamp":1736413064896,"user_tz":-330,"elapsed":79154,"user":{"displayName":"Arnav Sonavane","userId":"11795496963666040889"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.preprocessing import LabelEncoder\n","\n","try:\n","  df = pd.read_csv('company.csv')\n","  print(\"CSV data loaded successfully\")\n","except FileNotFoundError:\n","    print(\"Error: company.csv not found. Please make sure the file is in the current directory.\")\n","    exit()\n","\n","numerical_cols = ['PAT', 'ROE', 'D/E', 'GMP', 'open', 'close']\n","for col in numerical_cols:\n","    df[col] = pd.to_numeric(df[col], errors='coerce')\n","    median_val = df[col].median()\n","    df[col].fillna(median_val, inplace=True)\n","\n","numerical_cols = ['PAT', 'ROE', 'D/E', 'GMP', 'open', 'close']\n","for col in numerical_cols:\n","    df[col] = pd.to_numeric(df[col], errors='coerce')\n","    median_val = df[col].median()\n","    df[col].fillna(median_val, inplace=True)\n","\n","df['fresh'] = pd.to_numeric(df['fresh'], errors='coerce')\n","df['fresh'].fillna(0, inplace=True)\n","\n","df['gains'] = ((df['close'] - df['open']) / df['open']) * 100\n","\n","def categorize_gains(gain):\n","  if gain > 10:\n","      return 2\n","  elif gain > 0:\n","      return 1\n","  else:\n","    return 0\n","\n","df['gain_class'] = df['gains'].apply(categorize_gains)\n","\n","df['gains'] = df['gains'].replace([np.inf, -np.inf], 0)\n","\n","print(\"First few rows of the processed DataFrame:\")\n","print(df.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MF2aWW3URsPg","executionInfo":{"status":"ok","timestamp":1736413078430,"user_tz":-330,"elapsed":1665,"user":{"displayName":"Arnav Sonavane","userId":"11795496963666040889"}},"outputId":"f7f0804a-c4c1-41e6-d1a4-fe5e576e153f"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["CSV data loaded successfully\n","First few rows of the processed DataFrame:\n","                                              name  \\\n","0                  Indo Farm Equipment Limited IPO   \n","1  Unimech Aerospace and Manufacturing Limited IPO   \n","2                        Carraro India Limited IPO   \n","3              Senores Pharmaceuticals Limited IPO   \n","4                  Ventive Hospitality Limited IPO   \n","\n","                                                Link     PAT    ROE    D/E  \\\n","0  https://www.chittorgarh.com/ipo/indo-farm-equi...    4.16   5.13  0.850   \n","1  https://www.chittorgarh.com/ipo/unimech-aerosp...   27.85  53.53  0.320   \n","2  https://www.chittorgarh.com/ipo/carraro-india-...    3.50  17.69  0.580   \n","3  https://www.chittorgarh.com/ipo/senores-pharma...   15.25  23.60  1.070   \n","4  https://www.chittorgarh.com/ipo/ventive-hospit... -137.83  11.79  0.515   \n","\n","     GMP      OFS   fresh   open   close      gains  gain_class  \n","0   80.0   250.00   250.0  215.0   293.2  36.372093           2  \n","1  625.0    75.25   184.9  785.0  1485.0  89.171975           2  \n","2  -10.0  1250.00     0.0  704.0   682.0  -3.125000           0  \n","3  280.0    82.11   500.0  391.0   609.0  55.754476           2  \n","4   85.0      NaN  1600.0  643.0   749.0  16.485226           2  \n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-9-f31fb8760601>:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n","The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n","\n","For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n","\n","\n","  df[col].fillna(median_val, inplace=True)\n","<ipython-input-9-f31fb8760601>:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n","The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n","\n","For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n","\n","\n","  df[col].fillna(median_val, inplace=True)\n","<ipython-input-9-f31fb8760601>:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n","The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n","\n","For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n","\n","\n","  df['fresh'].fillna(0, inplace=True)\n"]}]},{"cell_type":"code","source":["import fitz\n","import json\n","from pathlib import Path\n","from doctr.io import DocumentFile\n","from doctr.models import ocr_predictor\n","import warnings\n","import os\n","from ultralytics import YOLO\n","from PIL import Image, ImageEnhance\n","from io import BytesIO\n","import pathlib\n","from pdf2image import convert_from_path\n","import google.generativeai as genai\n","from google.generativeai import GenerativeModel\n","\n","GOOGLE_API_KEY = \"AIzaSyBZzIGBcRvvRz2MYHAgU7MV4gXDu2k0VHU\"\n","\n","genai.configure(api_key=GOOGLE_API_KEY)\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","def enhance_image(img):\n","    enhancer = ImageEnhance.Sharpness(img)\n","    img = enhancer.enhance(1.5)\n","    enhancer = ImageEnhance.Contrast(img)\n","    img = enhancer.enhance(1.2)\n","    enhancer = ImageEnhance.Color(img)\n","    img = enhancer.enhance(1.1)\n","    return img\n","\n","def process_pdf_page(pdf_path, page_num, docseg_model, output_dir):\n","    pdf_doc = fitz.open(pdf_path)\n","    page = pdf_doc[page_num]\n","    zoom = 4\n","    matrix = fitz.Matrix(zoom, zoom)\n","    pix = page.get_pixmap(\n","        matrix=matrix,\n","        alpha=False,\n","        colorspace=fitz.csRGB\n","    )\n","    img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n","    img = enhance_image(img)\n","    if zoom != 1:\n","        original_size = (int(page.rect.width), int(page.rect.height))\n","        img = img.resize(original_size, Image.Resampling.LANCZOS)\n","    temp_img_filename = os.path.join(output_dir, f\"temp_page_{page_num}.png\")\n","    img.save(\n","        temp_img_filename,\n","        \"PNG\",\n","        quality=100,\n","        optimize=False,\n","        dpi=(300, 300)\n","    )\n","    results = docseg_model(source=temp_img_filename, save=True, show_labels=True, show_conf=True, boxes=True)\n","    page_width = page.rect.width\n","    one_third_width = page_width / 3\n","    all_coords = []\n","    for entry in results:\n","        thepath = pathlib.Path(entry.path)\n","        thecoords = entry.boxes.xyxy.numpy()\n","        all_coords.extend(thecoords)\n","    left_group = []\n","    right_group = []\n","    for bbox in all_coords:\n","            x1 = bbox[0]\n","            if x1 < one_third_width:\n","                left_group.append(bbox)\n","            else:\n","                right_group.append(bbox)\n","    left_group = sorted(left_group, key=lambda bbox: bbox[1])\n","    right_group = sorted(right_group, key=lambda bbox: bbox[1])\n","    sorted_coords = left_group + right_group\n","    mydict[f\"{pdf_path} Page {page_num}\"] = sorted_coords\n","    os.remove(temp_img_filename)\n","    pdf_doc.close()\n","\n","def extract_text_from_image(image_path, model):\n","    doc = DocumentFile.from_images(image_path)\n","    result = model(doc)\n","    text_content = \"\"\n","    for page in result.pages:\n","        for block in page.blocks:\n","            for line in block.lines:\n","                for word in line.words:\n","                    text_content += word.value + \" \"\n","            text_content += \"\\n\"\n","    return text_content.strip()\n","\n","def process_cropped_images(tmp_dir, pdf_list):\n","    doctr_model = ocr_predictor(pretrained=True)\n","    for pdf_path in pdf_list:\n","        pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n","        output_txt_path = f\"{pdf_name}_extracted_text.txt\"\n","        with open(output_txt_path, 'w', encoding='utf-8') as outfile:\n","            pdf_doc = fitz.open(pdf_path)\n","            num_pages = pdf_doc.page_count\n","            pdf_doc.close()\n","            for page_num in range(num_pages):\n","                outfile.write(f\"Page: {page_num}\\n\")\n","                cropped_images_for_page = sorted([\n","                    f for f in os.listdir(tmp_dir)\n","                    if f.startswith(f\"{pdf_name}_page{page_num}_\") and f.endswith(\".png\")\n","                ], key=lambda f: int(f.split(\"_\")[-1].split(\".\")[0]))\n","                for i, image_filename in enumerate(cropped_images_for_page):\n","                    image_path = os.path.join(tmp_dir, image_filename)\n","                    text = extract_text_from_image(image_path, doctr_model)\n","                    outfile.write(f\"  Chunk {i}: {text}\\n\")\n","        print(f\"Text extracted from {pdf_name} saved to {output_txt_path}\")\n","\n","def extract_text_from_pdf(pdf_path: Path) -> dict:\n","    try:\n","        pdf_document = fitz.open(pdf_path)\n","        if not pdf_document:\n","            return {}\n","        page_text = {}\n","        total_words = 0\n","        all_text_extracted = True\n","        for page_number in range(pdf_document.page_count):\n","            page = pdf_document[page_number]\n","            text = page.get_text()\n","            if not text.strip():\n","              all_text_extracted = False\n","              break\n","            word_count = len(text.split())\n","            total_words += word_count\n","            page_text[page_number + 1] = text\n","        pdf_document.close()\n","        if all_text_extracted and page_text and total_words / len(page_text) >= 10:\n","             return page_text\n","        print(\"Its a Scanned PDF, running OCR.\")\n","        doc = DocumentFile.from_pdf(str(pdf_path))\n","        model = ocr_predictor(pretrained=True)\n","        result = model(doc)\n","        page_text_doctr = {}\n","        for idx, page in enumerate(result.pages):\n","            text_content = \"\"\n","            for block in page.blocks:\n","                for line in block.lines:\n","                    for word in line.words:\n","                        text_content += word.value + \" \"\n","                text_content += \"\\n\"\n","            page_text_doctr[idx + 1] = text_content.strip()\n","        return page_text_doctr\n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")\n","        return {}\n","\n","def save_to_json(data: dict, output_path: Path):\n","    try:\n","        with open(output_path, \"w\", encoding=\"utf-8\") as json_file:\n","            json.dump(data, json_file, indent=4, ensure_ascii=False)\n","    except Exception as e:\n","        print(f\"Error saving to JSON: {e}\")\n","\n","def extract_text_gemini(pdf_path, prompt):\n","        pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n","        output_dir = \"GeminiVisionResult\"\n","        os.makedirs(output_dir, exist_ok=True)\n","        model = GenerativeModel(model_name=\"gemini-1.5-pro\")\n","        try:\n","            images = convert_from_path(pdf_path)\n","            if not images:\n","                raise FileNotFoundError(f\"Could not convert the PDF to images\")\n","            all_page_data = {}\n","            for i, img in enumerate(images):\n","                page_number = i + 1\n","                output_file_path = os.path.join(output_dir, f\"{pdf_name}_{page_number}.txt\")\n","                try:\n","                    response = model.generate_content([prompt, img], generation_config={\"max_output_tokens\": 4096})\n","                    response.resolve()\n","                    with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n","                      f.write(response.text)\n","                    all_page_data[f\"{pdf_path} Page {page_number}\"] = response.text\n","                    print(f\"Processed page {page_number} and saved to {output_file_path}\")\n","                except Exception as page_err:\n","                    print(f\"Error processing page {page_number}: {page_err}\")\n","                    with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n","                      f.write(f\"Error: An error occurred during processing of page {page_number} : {page_err}\")\n","            return all_page_data\n","        except FileNotFoundError as e:\n","            print(f\"Error: Could not find file: {e}\")\n","            return {}\n","        except Exception as e:\n","            print(f\"Error: An error occurred during processing: {e}\")\n","            return {}\n","\n","docseg_model = YOLO('yolov8x-doclaynet-epoch64-imgsz640-initiallr1e-4-finallr1e-5.pt')\n","mydict = {}\n","\n","text_extraction_mapping = {\n","    \"ratios pdf\": \"fitz\",\n","    \"Red Herring Prospectus\": \"doctr_yolo\",\n","    \"gemini_vision\":\"gemini\"\n","}\n","\n","def extract_text_for_row(row):\n","  pdf_link = row['Link']\n","  if not isinstance(pdf_link, str):\n","      print(f\"Warning: Invalid link format: {pdf_link}. Skipping text extraction.\")\n","      return \"\"\n","  try:\n","    if \"chittorgarh.com\" not in pdf_link:\n","      print(\"Warning: Invalid link domain. Text extraction skipped.\")\n","      return \"\"\n","    pdf_id = pdf_link.split(\"/\")[-2]\n","    pdf_type = \"Red Herring Prospectus\" if \"ipo\" in pdf_link else \"ratios pdf\"\n","    pdf_filename = f\"{pdf_id}_{pdf_type.replace(' ', '_')}.pdf\"\n","    if not os.path.exists(pdf_filename):\n","       print(f\"File '{pdf_filename}' not found. Downloading...\")\n","       try:\n","           import requests\n","           response = requests.get(pdf_link)\n","           response.raise_for_status()\n","           with open(pdf_filename, 'wb') as f:\n","               f.write(response.content)\n","           print(f\"Downloaded '{pdf_filename}'\")\n","       except requests.exceptions.RequestException as e:\n","         print(f\"Error downloading PDF: {e}\")\n","         return \"\"\n","    else:\n","        print(f\"File '{pdf_filename}' already exists\")\n","    extraction_method = text_extraction_mapping.get(pdf_type, \"fitz\")\n","    if extraction_method == \"fitz\":\n","         extracted_data = extract_text_from_pdf(Path(pdf_filename))\n","         if extracted_data:\n","           combined_text = \"\\\\n\".join(extracted_data.values())\n","           return combined_text\n","         else:\n","           print(\"fitz failed to extract\")\n","           return \"\"\n","    elif extraction_method == \"doctr_yolo\":\n","        output_dir = os.path.splitext(pdf_filename)[0] + \"_output\"\n","        os.makedirs(output_dir, exist_ok=True)\n","        pdf_doc = fitz.open(pdf_filename)\n","        num_pages = pdf_doc.page_count\n","        pdf_doc.close()\n","        for page_num in range(num_pages):\n","           process_pdf_page(pdf_filename, page_num, docseg_model, output_dir)\n","           tmp_dir = 'tmp'\n","           os.makedirs(tmp_dir, exist_ok=True)\n","        process_cropped_images(tmp_dir, [pdf_filename])\n","        extracted_text = \"\"\n","        try:\n","           with open(f\"{pdf_filename.replace('.pdf', '')}_extracted_text.txt\", 'r', encoding='utf-8') as f:\n","               extracted_text = f.read()\n","        except FileNotFoundError as e:\n","           print(f\"Error opening text file after docter: {e}\")\n","        return extracted_text\n","    elif extraction_method == \"gemini\":\n","         prompt = \"\"\"\n","             Extract all text content and tabular data from this image, strictly preserving the original reading order as they appear on the page.\n","             1. **Reading Order:** Process the content strictly based on the reading order within the image. Do not rearrange or reorder blocks or tables.\n","             2. **Text Blocks:** Extract distinct blocks of text and represent each block as a separate entity, separated by double newlines (\"\\\\n\\\\n\").\n","             3. **Tables:** Identify any tables present in the image. For each table, output it in a structured, comma-separated format (.csv). Each row of the table should be on a new line, with commas separating column values.\n","                 - Include the header row, if present.\n","                 - Ensure that all columns of each row are comma separated values.\n","             4. **Output Format:**\n","                 - Output text blocks and tables in the order they are read on the page. When a table is encountered while reading the page, output it in CSV format at that point in the output.\n","             5. If there are no text or no tables return empty string.\n","            If the table contains only one row, then return text of that row separated by comma.\n","             \"\"\"\n","         extracted_data = extract_text_gemini(pdf_filename, prompt)\n","         if extracted_data:\n","           combined_text = \"\\\\n\".join(extracted_data.values())\n","           return combined_text\n","         else:\n","           print(\"gemini failed to extract\")\n","           return \"\"\n","  except Exception as e:\n","      print(f\"An error occurred during extraction: {e}\")\n","      return \"\"\n","\n","df['extracted_text'] = df.apply(extract_text_for_row, axis=1)\n","\n","print(df[['Link', 'extracted_text']].head())"],"metadata":{"id":"z5DRnAalRt7Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_text(text):\n","    if not isinstance(text, str):\n","      return \"\"\n","    text = text.lower()\n","    return text\n","\n","df['processed_text'] = df['extracted_text'].apply(preprocess_text)\n","\n","text_data = df['processed_text'].tolist()\n","all_text = \" \".join(text_data)\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","vectorizer = TfidfVectorizer(max_features=200, stop_words='english')\n","\n","tfidf_matrix = vectorizer.fit_transform(text_data).toarray()\n","\n","tfidf_df = pd.DataFrame(tfidf_matrix, columns=vectorizer.get_feature_names_out())\n","\n","numerical_cols = ['PAT', 'ROE', 'D/E', 'GMP', 'fresh', 'open', 'close']\n","df_numerical = df[numerical_cols]\n","\n","df_combined = pd.concat([df_numerical.reset_index(drop=True), tfidf_df.reset_index(drop=True)], axis=1)\n","\n","X = df_combined\n","y = df['gain_class']\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","print(\"\\nData is prepared for model training.\")"],"metadata":{"id":"mxqHcwjZRwGX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import Pipeline\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","pipeline = Pipeline([\n","    ('scaler', StandardScaler()),\n","    ('classifier', RandomForestClassifier(random_state=42))\n","])\n","\n","pipeline.fit(X_train, y_train)"],"metadata":{"id":"ocmVJwG-RyIx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict_ipo_gain(new_data: pd.DataFrame):\n","    numerical_cols = ['PAT', 'ROE', 'D/E', 'GMP', 'open', 'close']\n","    for col in numerical_cols:\n","        new_data[col] = pd.to_numeric(new_data[col], errors='coerce')\n","        median_val = new_data[col].median()\n","        new_data[col].fillna(median_val, inplace=True)\n","    new_data['fresh'] = pd.to_numeric(new_data['fresh'], errors='coerce')\n","    new_data['fresh'].fillna(0, inplace=True)\n","    new_data['extracted_text'] = new_data.apply(extract_text_for_row, axis=1)\n","    new_data['processed_text'] = new_data['extracted_text'].apply(preprocess_text)\n","    new_text = new_data['processed_text'].tolist()\n","    new_tfidf_matrix = vectorizer.transform(new_text).toarray()\n","    new_tfidf_df = pd.DataFrame(new_tfidf_matrix, columns=vectorizer.get_feature_names_out())\n","    new_numerical = new_data[numerical_cols]\n","    new_combined = pd.concat([new_numerical.reset_index(drop=True), new_tfidf_df.reset_index(drop=True)], axis=1)\n","    gain_class = pipeline.predict(new_combined)\n","    return gain_class[0]\n","\n","new_data_example = pd.DataFrame([\n","    {'name': 'Example Company',\n","     'Link': 'https://www.chittorgarh.com/ipo/senores-pharmaceuticals-ipo/1943/',\n","     'PAT': 15.25,\n","     'ROE': 23.60,\n","     'D/E': 1.07,\n","     'GMP': 280,\n","     'fresh': 500.00,\n","     'open': 391.00,\n","     'close': 609.00},\n","    {'name': 'Example Company 2',\n","     'Link': 'https://www.chittorgarh.com/ipo/transrail-lighting-ipo/1933/',\n","     'PAT': 5.65,\n","     'ROE': 24.41,\n","     'D/E': 0.56,\n","     'GMP': 165,\n","     'fresh': 400,\n","      'open': 432,\n","     'close': 605},\n","     {'name': 'Example Company 3',\n","     'Link': 'https://www.chittorgarh.com/ipo/property-share-reit-ipo/1917/',\n","     'PAT': 10,\n","     'ROE': 11,\n","     'D/E': 0.5,\n","     'GMP': 10,\n","     'fresh': 0,\n","      'open': 1050000,\n","     'close': 1050050}\n","])\n","\n","predicted_gain_class = predict_ipo_gain(new_data_example)\n","print(\"\\nPredicted IPO gain class (0=negative, 1=moderate, 2=positive):\", predicted_gain_class)"],"metadata":{"id":"EqsqDGOlR0kv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred = pipeline.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Model Accuracy: {accuracy:.2f}\")\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, y_pred))"],"metadata":{"id":"JKLukXy6R1xI"},"execution_count":null,"outputs":[]}]}