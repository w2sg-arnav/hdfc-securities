{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import json\n",
    "from pathlib import Path\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from together import Together\n",
    "import PIL.Image\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from pdf2image import convert_from_path\n",
    "import time\n",
    "\n",
    "# Ensure you have these environment variables set\n",
    "TOGETHER_AI_API_KEY = \"64880c44ef37384040dc253c954ed2f190c0e4702c3e80745e5eb78221f47376\"\n",
    "PINECONE_API_KEY = \"pcsk_2aEGcj_7cwy95qcT59b57wGLdNgNquJdiTiBJXNU27UiEob5cisrASpM99fcBHPeHwxp4U\"\n",
    "PINECONE_ENVIRONMENT = \"us-east-1\"\n",
    "PINECONE_BASE_INDEX_NAME = \"rag-chatbot-index\"\n",
    "GOOGLE_API_KEY = \"AIzaSyBe7hdWbsCf6kQmyoMAUXbOlr7p8v1Tjhk\"\n",
    "\n",
    "def load_prompt(prompt_file: Path) -> str:\n",
    "    \"\"\"Loads the prompt from the given text file.\"\"\"\n",
    "    try:\n",
    "        with open(prompt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Prompt file not found at {prompt_file}. Using default prompt.\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading prompt from {prompt_file}: {e}. Using default prompt.\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file, page by page, using fitz.\n",
    "    If fitz extraction fails or extracts less than 20 words, it falls back to Gemini Vision API.\n",
    "\n",
    "    Args:\n",
    "      pdf_path: Path to the input PDF file.\n",
    "\n",
    "    Returns:\n",
    "      A dictionary where keys are page numbers (starting from 1) and values are the\n",
    "      extracted text from that page.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "        page_text = {}\n",
    "        for page_number in range(pdf_document.page_count):\n",
    "            page = pdf_document[page_number]\n",
    "            text = page.get_text()\n",
    "            page_text[page_number + 1] = text\n",
    "        pdf_document.close()\n",
    "\n",
    "        total_words = sum(len(text.split()) for text in page_text.values())\n",
    "        if total_words >= 20:\n",
    "            return page_text\n",
    "        else:\n",
    "            print(f\"fitz extracted less than 20 words ({total_words}). Falling back to Gemini Vision API.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during PDF extraction with fitz: {e}. Falling back to Gemini Vision API.\")\n",
    "\n",
    "    # Fallback to Gemini Vision API\n",
    "    try:\n",
    "        genai.configure(api_key=GOOGLE_API_KEY)\n",
    "        model = genai.GenerativeModel(model_name=\"gemini-1.5-pro\")\n",
    "        images = convert_from_path(pdf_path)\n",
    "        gemini_page_text = {}\n",
    "        pdf_name = os.path.splitext(os.path.basename(str(pdf_path)))[0]\n",
    "        output_dir = \"GeminiVisionResult\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        prompt_file_path = Path(\"prompt.txt\")\n",
    "        prompt = load_prompt(prompt_file_path)\n",
    "\n",
    "        if not images:\n",
    "            raise FileNotFoundError(f\"Could not convert the PDF to images\")\n",
    "\n",
    "        for i, img in enumerate(images):\n",
    "            page_number = i + 1\n",
    "            output_file_path = os.path.join(output_dir, f\"{pdf_name}_{page_number}.txt\")\n",
    "\n",
    "            try:\n",
    "                response = model.generate_content([prompt, img], generation_config={\"max_output_tokens\": 4096})\n",
    "                response.resolve()\n",
    "                gemini_page_text[page_number] = response.text\n",
    "                print(f\"Gemini processed page {page_number}\")\n",
    "            except Exception as page_err:\n",
    "                print(f\"Error processing page {page_number} with Gemini: {page_err}\")\n",
    "                gemini_page_text[page_number] = f\"Error: An error occurred during Gemini processing of page {page_number}: {page_err}\"\n",
    "        return gemini_page_text\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Could not find file: {e}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Gemini Vision API processing: {e}\")\n",
    "        return {}\n",
    "\n",
    "def semantic_chunking(text_dict: dict, chunk_size: int = 500, chunk_overlap: int = 50):\n",
    "    \"\"\"\n",
    "    Chunks the extracted text semantically.\n",
    "\n",
    "    Args:\n",
    "      text_dict: Dictionary of page-wise extracted text.\n",
    "      chunk_size: Maximum size of each chunk.\n",
    "      chunk_overlap: Number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "      A list of text chunks.\n",
    "    \"\"\"\n",
    "    all_text = \"\\n\".join(text_dict.values())\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "        length_function=len,\n",
    "    )\n",
    "    chunks = text_splitter.split_text(all_text)\n",
    "    return chunks\n",
    "\n",
    "def embed_and_upsert_to_pinecone(chunks: list[str], index):\n",
    "    \"\"\"\n",
    "    Embeds the text chunks using HuggingFace embeddings and upserts them to Pinecone.\n",
    "    \"\"\"\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\") # Choose an appropriate model\n",
    "\n",
    "    batch_size = 32\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch_chunks = chunks[i:i + batch_size]\n",
    "        ids = [f\"chunk-{i}-{j}\" for j in range(len(batch_chunks))]\n",
    "        embeds = embeddings.embed_documents(batch_chunks)\n",
    "        metadata = [{\"text\": text} for text in batch_chunks]\n",
    "        to_upsert = list(zip(ids, embeds, metadata))\n",
    "        index.upsert(vectors=to_upsert)\n",
    "    print(f\"Upserted {len(chunks)} chunks to Pinecone.\")\n",
    "\n",
    "def generate_response(query: str, context: str, model_name: str = \"meta-llama/Llama-3-8b-chat-hf\"):\n",
    "    \"\"\"\n",
    "    Generates a response using Together AI's API with the `together` library.\n",
    "\n",
    "    Args:\n",
    "      query: The user's question.\n",
    "      context: Retrieved relevant text from Pinecone.\n",
    "      model_name: The name of the Together AI model to use.\n",
    "\n",
    "    Returns:\n",
    "      The LLM's response.\n",
    "    \"\"\"\n",
    "    if not TOGETHER_AI_API_KEY:\n",
    "        raise ValueError(\"TOGETHER_AI_API_KEY environment variable not set.\")\n",
    "\n",
    "    client = Together(api_key=TOGETHER_AI_API_KEY)\n",
    "\n",
    "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=512,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def query_pinecone(query: str, index, top_k: int = 15):\n",
    "    \"\"\"\n",
    "    Queries Pinecone for relevant chunks.\n",
    "\n",
    "    Args:\n",
    "      query: The user's question.\n",
    "      index: The Pinecone index to query.\n",
    "      top_k: Number of relevant chunks to retrieve.\n",
    "\n",
    "    Returns:\n",
    "      A string containing the concatenated text of the top_k chunks.\n",
    "    \"\"\"\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
    "    query_vector = embeddings.embed_query(query)\n",
    "\n",
    "    results = index.query(vector=query_vector, top_k=top_k, include_values=False, include_metadata=True)\n",
    "    context = \"\\n\\n\".join([match.metadata[\"text\"] for match in results.matches])\n",
    "    return context\n",
    "\n",
    "class RAGChatbot:\n",
    "    def __init__(self, existing_index_name):\n",
    "        if not PINECONE_API_KEY:\n",
    "            raise ValueError(\"Pinecone API key must be set.\")\n",
    "        self.pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "        self.pinecone_index_name = existing_index_name\n",
    "        print(f\"Connecting to existing Pinecone index '{self.pinecone_index_name}'...\")\n",
    "        try:\n",
    "            self.index = self.pc.Index(self.pinecone_index_name)\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error connecting to Pinecone index '{self.pinecone_index_name}': {e}\")\n",
    "\n",
    "    def ingest_pdfs(self, pdf_paths: list[Path]):\n",
    "        \"\"\"Ingests a list of PDFs, chunks them, and uploads to Pinecone.\"\"\"\n",
    "        for pdf_path in pdf_paths:\n",
    "            print(f\"Processing PDF: {pdf_path}\")\n",
    "            extracted_text = extract_text_from_pdf(pdf_path)\n",
    "            if extracted_text:\n",
    "                chunks = semantic_chunking(extracted_text)\n",
    "                embed_and_upsert_to_pinecone(chunks, self.index)\n",
    "            else:\n",
    "                print(f\"No text extracted from the PDF: {pdf_path}\")\n",
    "\n",
    "    def query(self, query: str):\n",
    "        \"\"\"Queries the chatbot with a user's question.\"\"\"\n",
    "        context = query_pinecone(query, self.index)\n",
    "        if not context:\n",
    "            return \"No relevant information found in the document.\"\n",
    "        response = generate_response(query, context)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to existing Pinecone index 'rag-chatbot-index-1736531078'...\n",
      "Question: Tell me about NewMalayalam Steel Limited\n",
      "Answer: Based on the provided text, here's what I can tell you about NewMalayalam Steel Limited:\n",
      "\n",
      "1. **Incorporation**: NewMalayalam Steel Limited was incorporated on March 31, 2017 as a private limited company under the Companies Act, 2013, and was converted into a public limited company on December 15, 2023.\n",
      "2. **Business**: The company is engaged in the business of manufacturing steel products, including tubes, pipes, and tubesheets, at its manufacturing unit located at Mala, Thrissur, Kerala, India.\n",
      "3. **Promoters**: The company has six promoters: Vazhappily Davis Varghese, Divyakumar Jain, Ankur Jain, Mahendra Kumar Jain, Molly Varghese, and Cyriac Varghese.\n",
      "4. **Registered Office**: The company's registered office is located at Door No. 2/546/A & 2/546/B Mala, Pallipuram P O, Mala, Thrissur - 680 732, Kerala, India.\n",
      "5. **Corporate Office**: As of the date of the Red Herring Prospectus, the company does not have a corporate office.\n",
      "6. **Manufacturing Unit**: The company's manufacturing unit is located at Door No. 2/546/A & 2/546/B Mala, Pallipuram P O, Mala, Thrissur - 680 732, Kerala, India.\n",
      "7. **Solar Unit**: The company has a solar unit located at Survey No. 262/3-29 in Poyya Village and Kodungallur Taluk; Building No.2/546 of Poyya Panchayath, Thrissur – 680 732.\n",
      "8. **Track Record**: The company has a track record of over 7 years, having been incorporated in 2017 as a private limited company.\n",
      "9. **Issue**: The company is undertaking a fresh issue of equity shares to raise funds for its business expansion plans.\n",
      "10. **Risk Factors**: The company has identified various risk factors, including internal and external risks, which could impact its business operations and financial performance.\n",
      "\n",
      "I hope this information helps! Let me know if you have any further questions.\n"
     ]
    }
   ],
   "source": [
    "chatbot = RAGChatbot(existing_index_name=\"rag-chatbot-index-1736531078\")\n",
    "\n",
    "# You can now query the existing index\n",
    "user_query = \"Tell me about NewMalayalam Steel Limited\"\n",
    "response = chatbot.query(user_query)\n",
    "print(f\"Question: {user_query}\")\n",
    "print(f\"Answer: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
