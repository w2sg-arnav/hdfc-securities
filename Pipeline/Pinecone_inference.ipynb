{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import json\n",
    "from pathlib import Path\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from together import Together\n",
    "import PIL.Image\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from pdf2image import convert_from_path\n",
    "import time\n",
    "\n",
    "# Ensure you have these environment variables set\n",
    "TOGETHER_AI_API_KEY = \"64880c44ef37384040dc253c954ed2f190c0e4702c3e80745e5eb78221f47376\"\n",
    "PINECONE_API_KEY = \"pcsk_2aEGcj_7cwy95qcT59b57wGLdNgNquJdiTiBJXNU27UiEob5cisrASpM99fcBHPeHwxp4U\"\n",
    "PINECONE_ENVIRONMENT = \"us-east-1\"\n",
    "PINECONE_BASE_INDEX_NAME = \"rag-chatbot-index\"\n",
    "GOOGLE_API_KEY = \"AIzaSyBe7hdWbsCf6kQmyoMAUXbOlr7p8v1Tjhk\"\n",
    "\n",
    "def load_prompt(prompt_file: Path) -> str:\n",
    "    \"\"\"Loads the prompt from the given text file.\"\"\"\n",
    "    try:\n",
    "        with open(prompt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Prompt file not found at {prompt_file}. Using default prompt.\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading prompt from {prompt_file}: {e}. Using default prompt.\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file, page by page, using fitz.\n",
    "    If fitz extraction fails or extracts less than 20 words, it falls back to Gemini Vision API.\n",
    "\n",
    "    Args:\n",
    "      pdf_path: Path to the input PDF file.\n",
    "\n",
    "    Returns:\n",
    "      A dictionary where keys are page numbers (starting from 1) and values are the\n",
    "      extracted text from that page.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "        page_text = {}\n",
    "        for page_number in range(pdf_document.page_count):\n",
    "            page = pdf_document[page_number]\n",
    "            text = page.get_text()\n",
    "            page_text[page_number + 1] = text\n",
    "        pdf_document.close()\n",
    "\n",
    "        total_words = sum(len(text.split()) for text in page_text.values())\n",
    "        if total_words >= 20:\n",
    "            return page_text\n",
    "        else:\n",
    "            print(f\"fitz extracted less than 20 words ({total_words}). Falling back to Gemini Vision API.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during PDF extraction with fitz: {e}. Falling back to Gemini Vision API.\")\n",
    "\n",
    "    # Fallback to Gemini Vision API\n",
    "    try:\n",
    "        genai.configure(api_key=GOOGLE_API_KEY)\n",
    "        model = genai.GenerativeModel(model_name=\"gemini-1.5-pro\")\n",
    "        images = convert_from_path(pdf_path)\n",
    "        gemini_page_text = {}\n",
    "        pdf_name = os.path.splitext(os.path.basename(str(pdf_path)))[0]\n",
    "        output_dir = \"GeminiVisionResult\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        prompt_file_path = Path(\"prompt.txt\")\n",
    "        prompt = load_prompt(prompt_file_path)\n",
    "\n",
    "        if not images:\n",
    "            raise FileNotFoundError(f\"Could not convert the PDF to images\")\n",
    "\n",
    "        for i, img in enumerate(images):\n",
    "            page_number = i + 1\n",
    "            output_file_path = os.path.join(output_dir, f\"{pdf_name}_{page_number}.txt\")\n",
    "\n",
    "            try:\n",
    "                response = model.generate_content([prompt, img], generation_config={\"max_output_tokens\": 4096})\n",
    "                response.resolve()\n",
    "                gemini_page_text[page_number] = response.text\n",
    "                print(f\"Gemini processed page {page_number}\")\n",
    "            except Exception as page_err:\n",
    "                print(f\"Error processing page {page_number} with Gemini: {page_err}\")\n",
    "                gemini_page_text[page_number] = f\"Error: An error occurred during Gemini processing of page {page_number}: {page_err}\"\n",
    "        return gemini_page_text\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Could not find file: {e}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Gemini Vision API processing: {e}\")\n",
    "        return {}\n",
    "\n",
    "def semantic_chunking(text_dict: dict, chunk_size: int = 500, chunk_overlap: int = 50):\n",
    "    \"\"\"\n",
    "    Chunks the extracted text semantically.\n",
    "\n",
    "    Args:\n",
    "      text_dict: Dictionary of page-wise extracted text.\n",
    "      chunk_size: Maximum size of each chunk.\n",
    "      chunk_overlap: Number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "      A list of text chunks.\n",
    "    \"\"\"\n",
    "    all_text = \"\\n\".join(text_dict.values())\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "        length_function=len,\n",
    "    )\n",
    "    chunks = text_splitter.split_text(all_text)\n",
    "    return chunks\n",
    "\n",
    "def embed_and_upsert_to_pinecone(chunks: list[str], index):\n",
    "    \"\"\"\n",
    "    Embeds the text chunks using HuggingFace embeddings and upserts them to Pinecone.\n",
    "    \"\"\"\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\") # Choose an appropriate model\n",
    "\n",
    "    batch_size = 32\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch_chunks = chunks[i:i + batch_size]\n",
    "        ids = [f\"chunk-{i}-{j}\" for j in range(len(batch_chunks))]\n",
    "        embeds = embeddings.embed_documents(batch_chunks)\n",
    "        metadata = [{\"text\": text} for text in batch_chunks]\n",
    "        to_upsert = list(zip(ids, embeds, metadata))\n",
    "        index.upsert(vectors=to_upsert)\n",
    "    print(f\"Upserted {len(chunks)} chunks to Pinecone.\")\n",
    "\n",
    "def generate_response(query: str, context: str, model_name: str = \"meta-llama/Llama-3-8b-chat-hf\"):\n",
    "    \"\"\"\n",
    "    Generates a response using Together AI's API with the `together` library.\n",
    "\n",
    "    Args:\n",
    "      query: The user's question.\n",
    "      context: Retrieved relevant text from Pinecone.\n",
    "      model_name: The name of the Together AI model to use.\n",
    "\n",
    "    Returns:\n",
    "      The LLM's response.\n",
    "    \"\"\"\n",
    "    if not TOGETHER_AI_API_KEY:\n",
    "        raise ValueError(\"TOGETHER_AI_API_KEY environment variable not set.\")\n",
    "\n",
    "    client = Together(api_key=TOGETHER_AI_API_KEY)\n",
    "\n",
    "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=512,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def query_pinecone(query: str, index, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Queries Pinecone for relevant chunks.\n",
    "\n",
    "    Args:\n",
    "      query: The user's question.\n",
    "      index: The Pinecone index to query.\n",
    "      top_k: Number of relevant chunks to retrieve.\n",
    "\n",
    "    Returns:\n",
    "      A string containing the concatenated text of the top_k chunks.\n",
    "    \"\"\"\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
    "    query_vector = embeddings.embed_query(query)\n",
    "\n",
    "    results = index.query(vector=query_vector, top_k=top_k, include_values=False, include_metadata=True)\n",
    "    context = \"\\n\\n\".join([match.metadata[\"text\"] for match in results.matches])\n",
    "    return context\n",
    "\n",
    "class RAGChatbot:\n",
    "    def __init__(self, existing_index_name=\"rag-chatbot-index-1736437058\"):\n",
    "        if not PINECONE_API_KEY:\n",
    "            raise ValueError(\"Pinecone API key must be set.\")\n",
    "        self.pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "        self.pinecone_index_name = existing_index_name\n",
    "        print(f\"Connecting to existing Pinecone index '{self.pinecone_index_name}'...\")\n",
    "        try:\n",
    "            self.index = self.pc.Index(self.pinecone_index_name)\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error connecting to Pinecone index '{self.pinecone_index_name}': {e}\")\n",
    "\n",
    "    def ingest_pdfs(self, pdf_paths: list[Path]):\n",
    "        \"\"\"Ingests a list of PDFs, chunks them, and uploads to Pinecone.\"\"\"\n",
    "        for pdf_path in pdf_paths:\n",
    "            print(f\"Processing PDF: {pdf_path}\")\n",
    "            extracted_text = extract_text_from_pdf(pdf_path)\n",
    "            if extracted_text:\n",
    "                chunks = semantic_chunking(extracted_text)\n",
    "                embed_and_upsert_to_pinecone(chunks, self.index)\n",
    "            else:\n",
    "                print(f\"No text extracted from the PDF: {pdf_path}\")\n",
    "\n",
    "    def query(self, query: str):\n",
    "        \"\"\"Queries the chatbot with a user's question.\"\"\"\n",
    "        context = query_pinecone(query, self.index)\n",
    "        if not context:\n",
    "            return \"No relevant information found in the document.\"\n",
    "        response = generate_response(query, context)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to existing Pinecone index 'rag-chatbot-index-1736437058'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9676/3099074572.py:177: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
      "2025-01-10 01:16:31.649609: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736451991.664677    9676 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736451991.668704    9676 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-10 01:16:31.683868: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the main points of this document?\n",
      "Answer: The main points of this document appear to be:\n",
      "\n",
      "1. Compliance with listing and legal requirements: The document mentions compliance with listing and other legal requirements relating to financial statements.\n",
      "2. Financial statements: The document discusses the preparation and presentation of financial statements, including a summary of significant accounting policies and the basis of preparation and presentation.\n",
      "3. Management discussion and analysis: The document includes a management discussion and analysis of the financial condition and results of operations.\n",
      "4. Related party transactions: The document requires the disclosure of related party transactions and significant related party transactions.\n",
      "5. Audit and internal control: The document mentions management letters/letters of internal control weaknesses issued by statutory auditors, internal audit reports relating to internal control weaknesses, and the appointment, removal, and terms of remuneration of the chief internal auditor.\n",
      "6. Restated financial information: The document includes restated financial information, which has been prepared in accordance with Ind AS, and notes to the financial statements.\n",
      "7. Outstanding litigation and material developments: The document mentions outstanding litigation and material developments that may impact the company's financial position and results of operations.\n"
     ]
    }
   ],
   "source": [
    "chatbot = RAGChatbot(existing_index_name=\"rag-chatbot-index-1736437058\")\n",
    "\n",
    "# You can now query the existing index\n",
    "user_query = \"What are the main points of this document?\"\n",
    "response = chatbot.query(user_query)\n",
    "print(f\"Question: {user_query}\")\n",
    "print(f\"Answer: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
