{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pinecone import Pinecone\n",
    "\n",
    "# # Replace with your actual API key and environment\n",
    "# PINECONE_API_KEY = \"pcsk_2aEGcj_7cwy95qcT59b57wGLdNgNquJdiTiBJXNU27UiEob5cisrASpM99fcBHPeHwxp4U\"\n",
    "# PINECONE_ENVIRONMENT = \"us-east-1\"\n",
    "\n",
    "# def force_delete_all_indexes():\n",
    "#     \"\"\"Deletes all Pinecone indexes in the specified environment without confirmation.\"\"\"\n",
    "#     try:\n",
    "#         pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
    "#         index_list = pc.list_indexes()\n",
    "\n",
    "#         if not index_list:\n",
    "#             print(\"No Pinecone indexes found to delete.\")\n",
    "#             return\n",
    "\n",
    "#         print(\"Deleting all Pinecone indexes...\")\n",
    "#         for index in index_list:\n",
    "#             try:\n",
    "#                 pc.delete_index(index.name)  # Access the 'name' attribute\n",
    "#                 print(f\"Successfully deleted index: {index.name}\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error deleting index '{index.name}': {e}\")\n",
    "#         print(\"Deletion process completed.\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred while listing or deleting indexes: {e}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     force_delete_all_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import json\n",
    "from pathlib import Path\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from together import Together\n",
    "import PIL.Image\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from pdf2image import convert_from_path\n",
    "import time\n",
    "\n",
    "# Ensure you have these environment variables set\n",
    "TOGETHER_AI_API_KEY = \"64880c44ef37384040dc253c954ed2f190c0e4702c3e80745e5eb78221f47376\"  \n",
    "PINECONE_API_KEY = \"pcsk_2aEGcj_7cwy95qcT59b57wGLdNgNquJdiTiBJXNU27UiEob5cisrASpM99fcBHPeHwxp4U\"\n",
    "PINECONE_ENVIRONMENT = \"us-east-1\"\n",
    "PINECONE_BASE_INDEX_NAME = \"rag-chatbot-index\"  \n",
    "GOOGLE_API_KEY = \"AIzaSyBe7hdWbsCf6kQmyoMAUXbOlr7p8v1Tjhk\"\n",
    "\n",
    "def load_prompt(prompt_file: Path) -> str:\n",
    "    \"\"\"Loads the prompt from the given text file.\"\"\"\n",
    "    try:\n",
    "        with open(prompt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Prompt file not found at {prompt_file}. Using default prompt.\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading prompt from {prompt_file}: {e}. Using default prompt.\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file, page by page, using fitz.\n",
    "    If fitz extraction fails or extracts less than 20 words, it falls back to Gemini Vision API.\n",
    "\n",
    "    Args:\n",
    "      pdf_path: Path to the input PDF file.\n",
    "\n",
    "    Returns:\n",
    "      A dictionary where keys are page numbers (starting from 1) and values are the\n",
    "      extracted text from that page.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "        page_text = {}\n",
    "        for page_number in range(pdf_document.page_count):\n",
    "            page = pdf_document[page_number]\n",
    "            text = page.get_text()\n",
    "            page_text[page_number + 1] = text\n",
    "        pdf_document.close()\n",
    "\n",
    "        total_words = sum(len(text.split()) for text in page_text.values())\n",
    "        if total_words >= 20:\n",
    "            return page_text\n",
    "        else:\n",
    "            print(f\"fitz extracted less than 20 words ({total_words}). Falling back to Gemini Vision API.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during PDF extraction with fitz: {e}. Falling back to Gemini Vision API.\")\n",
    "\n",
    "    # Fallback to Gemini Vision API\n",
    "    try:\n",
    "        genai.configure(api_key=GOOGLE_API_KEY)\n",
    "        model = genai.GenerativeModel(model_name=\"gemini-1.5-pro\")\n",
    "        images = convert_from_path(pdf_path)\n",
    "        gemini_page_text = {}\n",
    "        pdf_name = os.path.splitext(os.path.basename(str(pdf_path)))[0]\n",
    "        output_dir = \"GeminiVisionResult\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        prompt_file_path = Path(\"prompt.txt\")\n",
    "        prompt = load_prompt(prompt_file_path)\n",
    "\n",
    "        if not images:\n",
    "            raise FileNotFoundError(f\"Could not convert the PDF to images\")\n",
    "\n",
    "        for i, img in enumerate(images):\n",
    "            page_number = i + 1\n",
    "            output_file_path = os.path.join(output_dir, f\"{pdf_name}_{page_number}.txt\")\n",
    "\n",
    "            try:\n",
    "                response = model.generate_content([prompt, img], generation_config={\"max_output_tokens\": 4096})\n",
    "                response.resolve()\n",
    "                gemini_page_text[page_number] = response.text\n",
    "                print(f\"Gemini processed page {page_number}\")\n",
    "            except Exception as page_err:\n",
    "                print(f\"Error processing page {page_number} with Gemini: {page_err}\")\n",
    "                gemini_page_text[page_number] = f\"Error: An error occurred during Gemini processing of page {page_number}: {page_err}\"\n",
    "        return gemini_page_text\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Could not find file: {e}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Gemini Vision API processing: {e}\")\n",
    "        return {}\n",
    "\n",
    "def semantic_chunking(text_dict: dict, chunk_size: int = 500, chunk_overlap: int = 50):\n",
    "    \"\"\"\n",
    "    Chunks the extracted text semantically.\n",
    "\n",
    "    Args:\n",
    "      text_dict: Dictionary of page-wise extracted text.\n",
    "      chunk_size: Maximum size of each chunk.\n",
    "      chunk_overlap: Number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "      A list of text chunks.\n",
    "    \"\"\"\n",
    "    all_text = \"\\n\".join(text_dict.values())\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "        length_function=len,\n",
    "    )\n",
    "    chunks = text_splitter.split_text(all_text)\n",
    "    return chunks\n",
    "\n",
    "def embed_and_upsert_to_pinecone(chunks: list[str], index):\n",
    "    \"\"\"\n",
    "    Embeds the text chunks using HuggingFace embeddings and upserts them to Pinecone.\n",
    "    \"\"\"\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L12-v2\") # Choose an appropriate model\n",
    "\n",
    "    batch_size = 32\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch_chunks = chunks[i:i + batch_size]\n",
    "        ids = [f\"chunk-{i}-{j}\" for j in range(len(batch_chunks))]\n",
    "        embeds = embeddings.embed_documents(batch_chunks)\n",
    "        metadata = [{\"text\": text} for text in batch_chunks]\n",
    "        to_upsert = list(zip(ids, embeds, metadata))\n",
    "        index.upsert(vectors=to_upsert)\n",
    "    print(f\"Upserted {len(chunks)} chunks to Pinecone.\")\n",
    "\n",
    "def generate_response(query: str, context: str, model_name: str = \"meta-llama/Llama-3-8b-chat-hf\"):\n",
    "    \"\"\"\n",
    "    Generates a response using Together AI's API with the `together` library.\n",
    "\n",
    "    Args:\n",
    "      query: The user's question.\n",
    "      context: Retrieved relevant text from Pinecone.\n",
    "      model_name: The name of the Together AI model to use.\n",
    "\n",
    "    Returns:\n",
    "      The LLM's response.\n",
    "    \"\"\"\n",
    "    if not TOGETHER_AI_API_KEY:\n",
    "        raise ValueError(\"TOGETHER_AI_API_KEY environment variable not set.\")\n",
    "\n",
    "    client = Together(api_key=TOGETHER_AI_API_KEY)\n",
    "\n",
    "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=512,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def query_pinecone(query: str, index, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Queries Pinecone for relevant chunks.\n",
    "\n",
    "    Args:\n",
    "      query: The user's question.\n",
    "      index: The Pinecone index to query.\n",
    "      top_k: Number of relevant chunks to retrieve.\n",
    "\n",
    "    Returns:\n",
    "      A string containing the concatenated text of the top_k chunks.\n",
    "    \"\"\"\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L12-v2\")\n",
    "    query_vector = embeddings.embed_query(query)\n",
    "\n",
    "    results = index.query(vector=query_vector, top_k=top_k, include_values=False, include_metadata=True)\n",
    "    context = \"\\n\\n\".join([match.metadata[\"text\"] for match in results.matches])\n",
    "    return context\n",
    "\n",
    "class RAGChatbot:\n",
    "    def __init__(self):\n",
    "        if not PINECONE_API_KEY:\n",
    "            raise ValueError(\"Pinecone API key must be set.\")\n",
    "        self.pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "        self.pinecone_index_name = f\"{PINECONE_BASE_INDEX_NAME}-{int(time.time())}\"\n",
    "        print(f\"Creating Pinecone index '{self.pinecone_index_name}'...\")\n",
    "        try:\n",
    "            self.pc.create_index(\n",
    "                name=self.pinecone_index_name,\n",
    "                dimension=384,  # Dimension of all-mpnet-base-v2 embeddings\n",
    "                metric=\"cosine\",\n",
    "                spec=ServerlessSpec(\n",
    "                    cloud=\"aws\",\n",
    "                    region=PINECONE_ENVIRONMENT\n",
    "                )\n",
    "            )\n",
    "            self.index = self.pc.Index(self.pinecone_index_name)\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error creating Pinecone index: {e}\")\n",
    "\n",
    "    def ingest_pdfs(self, pdf_paths: list[Path]):\n",
    "        \"\"\"Ingests a list of PDFs, chunks them, and uploads to Pinecone.\"\"\"\n",
    "        for pdf_path in pdf_paths:\n",
    "            print(f\"Processing PDF: {pdf_path}\")\n",
    "            extracted_text = extract_text_from_pdf(pdf_path)\n",
    "            if extracted_text:\n",
    "                chunks = semantic_chunking(extracted_text)\n",
    "                embed_and_upsert_to_pinecone(chunks, self.index)\n",
    "            else:\n",
    "                print(f\"No text extracted from the PDF: {pdf_path}\")\n",
    "\n",
    "    def query(self, query: str):\n",
    "        \"\"\"Queries the chatbot with a user's question.\"\"\"\n",
    "        context = query_pinecone(query, self.index)\n",
    "        if not context:\n",
    "            return \"No relevant information found in the document.\"\n",
    "        response = generate_response(query, context)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Pinecone index 'rag-chatbot-index-1736522466'...\n",
      "Processing PDF: ../RHP_Documents/Abha Power and Steel_RHP.pdf\n",
      "Upserted 2235 chunks to Pinecone.\n",
      "Processing PDF: ../RHP_Documents/Aditya Ultra Steel_RHP.pdf\n",
      "Upserted 347 chunks to Pinecone.\n",
      "Processing PDF: ../RHP_Documents/Aeron Composite_RHP.pdf\n",
      "Upserted 2775 chunks to Pinecone.\n",
      "Processing PDF: ../RHP_Documents/Apex Ecotech_RHP.pdf\n",
      "Upserted 2250 chunks to Pinecone.\n",
      "Processing PDF: ../RHP_Documents/Arkade Developers_RHP.pdf\n",
      "Upserted 4030 chunks to Pinecone.\n",
      "Processing PDF: ../RHP_Documents/Baazar Style Retail_RHP.pdf\n",
      "Upserted 4144 chunks to Pinecone.\n",
      "Processing PDF: ../RHP_Documents/Blackbuck_RHP.pdf\n",
      "Upserted 4115 chunks to Pinecone.\n",
      "Processing PDF: ../RHP_Documents/Boss Packaging Solutions_RHP.pdf\n",
      "Upserted 2114 chunks to Pinecone.\n",
      "Processing PDF: ../RHP_Documents/C2C Advanced Systems_RHP.pdf\n",
      "Upserted 163 chunks to Pinecone.\n",
      "Processing PDF: ../RHP_Documents/Concord Enviro Systems Limited_RHP.pdf\n",
      "Upserted 5654 chunks to Pinecone.\n",
      "Processing PDF: ../RHP_Documents/DAM Capital Advisors Limited_RHP.pdf\n",
      "Upserted 3513 chunks to Pinecone.\n",
      "Processing PDF: ../RHP_Documents/Danish Power_RHP.pdf\n",
      "Upserted 2866 chunks to Pinecone.\n",
      "Processing PDF: ../RHP_Documents/Deccan Transcon Leasing_RHP.pdf\n",
      "Upserted 2498 chunks to Pinecone.\n",
      "Processing PDF: ../RHP_Documents/Dhanlaxmi Crop Science_RHP.pdf\n",
      "Upserted 3245 chunks to Pinecone.\n",
      "Processing PDF: ../RHP_Documents/Diffusion Engineers Ltd_RHP.pdf\n",
      "Upserted 3679 chunks to Pinecone.\n",
      "Pinecone setup and ready for querying.\n"
     ]
    }
   ],
   "source": [
    "pdf_files = [\n",
    "    Path('../RHP_Documents/Abha Power and Steel_RHP.pdf'),\n",
    "    Path('../RHP_Documents/Aditya Ultra Steel_RHP.pdf'),\n",
    "    Path('../RHP_Documents/Aeron Composite_RHP.pdf'),\n",
    "    Path('../RHP_Documents/Apex Ecotech_RHP.pdf'),\n",
    "    Path('../RHP_Documents/Arkade Developers_RHP.pdf'),\n",
    "    Path('../RHP_Documents/Baazar Style Retail_RHP.pdf'),\n",
    "    Path('../RHP_Documents/Blackbuck_RHP.pdf'),\n",
    "    Path('../RHP_Documents/Boss Packaging Solutions_RHP.pdf'),\n",
    "    Path('../RHP_Documents/C2C Advanced Systems_RHP.pdf'),\n",
    "    Path('../RHP_Documents/Concord Enviro Systems Limited_RHP.pdf'),\n",
    "    Path('../RHP_Documents/DAM Capital Advisors Limited_RHP.pdf'),\n",
    "    Path('../RHP_Documents/Danish Power_RHP.pdf'),\n",
    "    Path('../RHP_Documents/Deccan Transcon Leasing_RHP.pdf'),\n",
    "    Path('../RHP_Documents/Dhanlaxmi Crop Science_RHP.pdf'),\n",
    "    Path('../RHP_Documents/Diffusion Engineers Ltd_RHP.pdf'),\n",
    "]\n",
    "\n",
    "chatbot = RAGChatbot()\n",
    "\n",
    "chatbot.ingest_pdfs(pdf_files)\n",
    "print(\"Pinecone setup and ready for querying.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "# Start an interactive chat session\n",
    "while True:\n",
    "    user_query = input(\"You: \")\n",
    "    if user_query.lower() == \"exit\":\n",
    "        break\n",
    "    response = chatbot.query(user_query)\n",
    "    print(f\"User: {user_query}\")\n",
    "    print(f\"Chatbot: {response}\")\n",
    "    print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
