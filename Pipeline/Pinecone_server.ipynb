{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all Pinecone indexes...\n",
      "Successfully deleted index: rag-chatbot-index-1736432525\n",
      "Deletion process completed.\n"
     ]
    }
   ],
   "source": [
    "# from pinecone import Pinecone\n",
    "\n",
    "# # Replace with your actual API key and environment\n",
    "# PINECONE_API_KEY = \"pcsk_2aEGcj_7cwy95qcT59b57wGLdNgNquJdiTiBJXNU27UiEob5cisrASpM99fcBHPeHwxp4U\"\n",
    "# PINECONE_ENVIRONMENT = \"us-east-1\"\n",
    "\n",
    "# def force_delete_all_indexes():\n",
    "#     \"\"\"Deletes all Pinecone indexes in the specified environment without confirmation.\"\"\"\n",
    "#     try:\n",
    "#         pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
    "#         index_list = pc.list_indexes()\n",
    "\n",
    "#         if not index_list:\n",
    "#             print(\"No Pinecone indexes found to delete.\")\n",
    "#             return\n",
    "\n",
    "#         print(\"Deleting all Pinecone indexes...\")\n",
    "#         for index in index_list:\n",
    "#             try:\n",
    "#                 pc.delete_index(index.name)  # Access the 'name' attribute\n",
    "#                 print(f\"Successfully deleted index: {index.name}\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error deleting index '{index.name}': {e}\")\n",
    "#         print(\"Deletion process completed.\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred while listing or deleting indexes: {e}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     force_delete_all_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import json\n",
    "from pathlib import Path\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from together import Together\n",
    "import PIL.Image\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from pdf2image import convert_from_path\n",
    "import time\n",
    "\n",
    "# Ensure you have these environment variables set\n",
    "TOGETHER_AI_API_KEY = \"64880c44ef37384040dc253c954ed2f190c0e4702c3e80745e5eb78221f47376\"  \n",
    "PINECONE_API_KEY = \"pcsk_2aEGcj_7cwy95qcT59b57wGLdNgNquJdiTiBJXNU27UiEob5cisrASpM99fcBHPeHwxp4U\"\n",
    "PINECONE_ENVIRONMENT = \"us-east-1\"\n",
    "PINECONE_BASE_INDEX_NAME = \"rag-chatbot-index\"  \n",
    "GOOGLE_API_KEY = \"AIzaSyBe7hdWbsCf6kQmyoMAUXbOlr7p8v1Tjhk\"\n",
    "\n",
    "def load_prompt(prompt_file: Path) -> str:\n",
    "    \"\"\"Loads the prompt from the given text file.\"\"\"\n",
    "    try:\n",
    "        with open(prompt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Prompt file not found at {prompt_file}. Using default prompt.\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading prompt from {prompt_file}: {e}. Using default prompt.\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file, page by page, using fitz.\n",
    "    If fitz extraction fails or extracts less than 20 words, it falls back to Gemini Vision API.\n",
    "\n",
    "    Args:\n",
    "      pdf_path: Path to the input PDF file.\n",
    "\n",
    "    Returns:\n",
    "      A dictionary where keys are page numbers (starting from 1) and values are the\n",
    "      extracted text from that page.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "        page_text = {}\n",
    "        for page_number in range(pdf_document.page_count):\n",
    "            page = pdf_document[page_number]\n",
    "            text = page.get_text()\n",
    "            page_text[page_number + 1] = text\n",
    "        pdf_document.close()\n",
    "\n",
    "        total_words = sum(len(text.split()) for text in page_text.values())\n",
    "        if total_words >= 20:\n",
    "            return page_text\n",
    "        else:\n",
    "            print(f\"fitz extracted less than 20 words ({total_words}). Falling back to Gemini Vision API.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during PDF extraction with fitz: {e}. Falling back to Gemini Vision API.\")\n",
    "\n",
    "    # Fallback to Gemini Vision API\n",
    "    try:\n",
    "        genai.configure(api_key=GOOGLE_API_KEY)\n",
    "        model = genai.GenerativeModel(model_name=\"gemini-1.5-pro\")\n",
    "        images = convert_from_path(pdf_path)\n",
    "        gemini_page_text = {}\n",
    "        pdf_name = os.path.splitext(os.path.basename(str(pdf_path)))[0]\n",
    "        output_dir = \"GeminiVisionResult\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        prompt_file_path = Path(\"prompt.txt\")\n",
    "        prompt = load_prompt(prompt_file_path)\n",
    "\n",
    "        if not images:\n",
    "            raise FileNotFoundError(f\"Could not convert the PDF to images\")\n",
    "\n",
    "        for i, img in enumerate(images):\n",
    "            page_number = i + 1\n",
    "            output_file_path = os.path.join(output_dir, f\"{pdf_name}_{page_number}.txt\")\n",
    "\n",
    "            try:\n",
    "                response = model.generate_content([prompt, img], generation_config={\"max_output_tokens\": 4096})\n",
    "                response.resolve()\n",
    "                gemini_page_text[page_number] = response.text\n",
    "                print(f\"Gemini processed page {page_number}\")\n",
    "            except Exception as page_err:\n",
    "                print(f\"Error processing page {page_number} with Gemini: {page_err}\")\n",
    "                gemini_page_text[page_number] = f\"Error: An error occurred during Gemini processing of page {page_number}: {page_err}\"\n",
    "        return gemini_page_text\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Could not find file: {e}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Gemini Vision API processing: {e}\")\n",
    "        return {}\n",
    "\n",
    "def semantic_chunking(text_dict: dict, chunk_size: int = 500, chunk_overlap: int = 50):\n",
    "    \"\"\"\n",
    "    Chunks the extracted text semantically.\n",
    "\n",
    "    Args:\n",
    "      text_dict: Dictionary of page-wise extracted text.\n",
    "      chunk_size: Maximum size of each chunk.\n",
    "      chunk_overlap: Number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "      A list of text chunks.\n",
    "    \"\"\"\n",
    "    all_text = \"\\n\".join(text_dict.values())\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "        length_function=len,\n",
    "    )\n",
    "    chunks = text_splitter.split_text(all_text)\n",
    "    return chunks\n",
    "\n",
    "def embed_and_upsert_to_pinecone(chunks: list[str], index):\n",
    "    \"\"\"\n",
    "    Embeds the text chunks using HuggingFace embeddings and upserts them to Pinecone.\n",
    "    \"\"\"\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\") # Choose an appropriate model\n",
    "\n",
    "    batch_size = 32\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch_chunks = chunks[i:i + batch_size]\n",
    "        ids = [f\"chunk-{i}-{j}\" for j in range(len(batch_chunks))]\n",
    "        embeds = embeddings.embed_documents(batch_chunks)\n",
    "        metadata = [{\"text\": text} for text in batch_chunks]\n",
    "        to_upsert = list(zip(ids, embeds, metadata))\n",
    "        index.upsert(vectors=to_upsert)\n",
    "    print(f\"Upserted {len(chunks)} chunks to Pinecone.\")\n",
    "\n",
    "def generate_response(query: str, context: str, model_name: str = \"meta-llama/Llama-3-8b-chat-hf\"):\n",
    "    \"\"\"\n",
    "    Generates a response using Together AI's API with the `together` library.\n",
    "\n",
    "    Args:\n",
    "      query: The user's question.\n",
    "      context: Retrieved relevant text from Pinecone.\n",
    "      model_name: The name of the Together AI model to use.\n",
    "\n",
    "    Returns:\n",
    "      The LLM's response.\n",
    "    \"\"\"\n",
    "    if not TOGETHER_AI_API_KEY:\n",
    "        raise ValueError(\"TOGETHER_AI_API_KEY environment variable not set.\")\n",
    "\n",
    "    client = Together(api_key=TOGETHER_AI_API_KEY)\n",
    "\n",
    "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=512,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def query_pinecone(query: str, index, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Queries Pinecone for relevant chunks.\n",
    "\n",
    "    Args:\n",
    "      query: The user's question.\n",
    "      index: The Pinecone index to query.\n",
    "      top_k: Number of relevant chunks to retrieve.\n",
    "\n",
    "    Returns:\n",
    "      A string containing the concatenated text of the top_k chunks.\n",
    "    \"\"\"\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
    "    query_vector = embeddings.embed_query(query)\n",
    "\n",
    "    results = index.query(vector=query_vector, top_k=top_k, include_values=False, include_metadata=True)\n",
    "    context = \"\\n\\n\".join([match.metadata[\"text\"] for match in results.matches])\n",
    "    return context\n",
    "\n",
    "class RAGChatbot:\n",
    "    def __init__(self):\n",
    "        if not PINECONE_API_KEY:\n",
    "            raise ValueError(\"Pinecone API key must be set.\")\n",
    "        self.pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "        self.pinecone_index_name = f\"{PINECONE_BASE_INDEX_NAME}-{int(time.time())}\"\n",
    "        print(f\"Creating Pinecone index '{self.pinecone_index_name}'...\")\n",
    "        try:\n",
    "            self.pc.create_index(\n",
    "                name=self.pinecone_index_name,\n",
    "                dimension=768,  # Dimension of all-mpnet-base-v2 embeddings\n",
    "                metric=\"cosine\",\n",
    "                spec=ServerlessSpec(\n",
    "                    cloud=\"aws\",\n",
    "                    region=PINECONE_ENVIRONMENT\n",
    "                )\n",
    "            )\n",
    "            self.index = self.pc.Index(self.pinecone_index_name)\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error creating Pinecone index: {e}\")\n",
    "\n",
    "    def ingest_pdfs(self, pdf_paths: list[Path]):\n",
    "        \"\"\"Ingests a list of PDFs, chunks them, and uploads to Pinecone.\"\"\"\n",
    "        for pdf_path in pdf_paths:\n",
    "            print(f\"Processing PDF: {pdf_path}\")\n",
    "            extracted_text = extract_text_from_pdf(pdf_path)\n",
    "            if extracted_text:\n",
    "                chunks = semantic_chunking(extracted_text)\n",
    "                embed_and_upsert_to_pinecone(chunks, self.index)\n",
    "            else:\n",
    "                print(f\"No text extracted from the PDF: {pdf_path}\")\n",
    "\n",
    "    def query(self, query: str):\n",
    "        \"\"\"Queries the chatbot with a user's question.\"\"\"\n",
    "        context = query_pinecone(query, self.index)\n",
    "        if not context:\n",
    "            return \"No relevant information found in the document.\"\n",
    "        response = generate_response(query, context)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Pinecone index 'rag-chatbot-index-1736437058'...\n",
      "Processing PDF: Annexure_A_Red_Herring_Prospectus.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16847/2927063626.py:125: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\") # Choose an appropriate model\n",
      "2025-01-09 21:07:54.166349: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736437074.182722   16847 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736437074.187162   16847 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-09 21:07:54.204739: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted 3762 chunks to Pinecone.\n",
      "Processing PDF: Annexure_I_Price_Band_Advertisement.pdf\n",
      "fitz extracted less than 20 words (0). Falling back to Gemini Vision API.\n",
      "Gemini processed page 1\n",
      "Gemini processed page 2\n",
      "Gemini processed page 3\n",
      "Gemini processed page 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yash/miniconda3/envs/myenv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted 123 chunks to Pinecone.\n",
      "Pinecone setup and ready for querying.\n"
     ]
    }
   ],
   "source": [
    "pdf_files = [\n",
    "    Path(\"Annexure_A_Red_Herring_Prospectus.pdf\"),\n",
    "    Path(\"Annexure_I_Price_Band_Advertisement.pdf\"),\n",
    "]\n",
    "\n",
    "chatbot = RAGChatbot()\n",
    "\n",
    "chatbot.ingest_pdfs(pdf_files)\n",
    "print(\"Pinecone setup and ready for querying.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "User: How many shares are owned by Rupinder Singh\n",
      "Chatbot: According to the data, Rupinder Singh owns:\n",
      "\n",
      "* 38,00,000 shares (mentioned in the table)\n",
      "* 3,00,000 shares (mentioned in the table)\n",
      "* 2,00,000 shares (mentioned in the table)\n",
      "* 2,00,000 shares (mentioned in the table)\n",
      "\n",
      "Total shares owned by Rupinder Singh: 45,00,000\n",
      "==================================================\n",
      "User: Tell me details about Working capital intensive businesses.\n",
      "Chatbot: A working capital intensive business is an enterprise that requires a significant amount of working capital to finance its day-to-day operations, inventory, and accounts receivable. Here are some key characteristics of working capital intensive businesses:\n",
      "\n",
      "1. **High inventory levels**: These businesses typically hold large amounts of inventory, which requires a significant investment of working capital.\n",
      "2. **Long payment cycles**: Working capital intensive businesses often have to wait a long time to receive payment from their customers, which can lead to cash flow problems.\n",
      "3. **High accounts receivable**: These businesses may have large amounts of outstanding accounts receivable, which can tie up a significant amount of working capital.\n",
      "4. **Frequent inventory replenishment**: Working capital intensive businesses often need to replenish their inventory frequently to meet customer demand, which requires additional working capital.\n",
      "5. **Rapid inventory turnover**: To maintain optimal inventory levels, these businesses may need to maintain a high inventory turnover rate, which can be challenging.\n",
      "6. **High working capital requirements**: Working capital intensive businesses typically require a significant amount of working capital to finance their operations, which can be a challenge for smaller or less established companies.\n",
      "7. **Cash flow volatility**: The cash flow of working capital intensive businesses can be volatile, with fluctuations in accounts receivable and inventory levels affecting cash flow.\n",
      "8. **Dependence on external financing**: Many working capital intensive businesses rely on external financing, such as bank loans or lines of credit, to fund their operations.\n",
      "\n",
      "Examples of working capital intensive businesses include:\n",
      "\n",
      "1. Retailers with large inventories of products\n",
      "2. Manufacturers with long production cycles and high inventory levels\n",
      "3. Companies with complex supply chains and long payment cycles\n",
      "4. Businesses with high levels of accounts receivable, such as those in the construction or consulting industries\n",
      "5. Companies that require frequent inventory replenishment, such as those in the food or beverage industry\n",
      "\n",
      "In the context of the company mentioned in the text, they are a working capital intensive business because they require a significant amount of working capital to finance their inventory, accounts receivable, and other operational requirements. They also have a high working capital requirement and may require alternative funding in the future to meet their working capital needs.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "# Start an interactive chat session\n",
    "while True:\n",
    "    user_query = input(\"You: \")\n",
    "    if user_query.lower() == \"exit\":\n",
    "        break\n",
    "    response = chatbot.query(user_query)\n",
    "    print(f\"User: {user_query}\")\n",
    "    print(f\"Chatbot: {response}\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
