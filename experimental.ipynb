{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDFs found in the directory: ['Abha Power and Steel_RHP.pdf', 'Aditya Ultra Steel_RHP.pdf', 'Aeron Composite_RHP.pdf', 'Apex Ecotech_RHP.pdf', 'Arkade Developers_RHP.pdf', 'Baazar Style Retail_RHP.pdf', 'Blackbuck_RHP.pdf', 'Boss Packaging Solutions_RHP.pdf', 'C2C Advanced Systems_RHP.pdf', 'Concord Enviro Systems Limited_RHP.pdf', 'DAM Capital Advisors Limited_RHP.pdf', 'Danish Power_RHP.pdf', 'Deccan Transcon Leasing_RHP.pdf', 'Dhanlaxmi Crop Science_RHP.pdf', 'Diffusion Engineers Ltd_RHP.pdf', 'Divyadhan Recycling Industries_RHP.pdf', 'ECO Mobility_RHP.pdf', 'Emerald Tyre Manufacturers_RHP.pdf', 'Enviro Infra Engineers_RHP.pdf', 'Envirotech Systems_RHP.pdf', 'Excellent Wires and Packaging_RHP.pdf', 'Forge Auto International_RHP.pdf', 'Freshara Agro Exports_RHP.pdf', 'Gajanand International_RHP.pdf', 'Gala Precision Engineering_RHP.pdf', 'Ganesh Infraworld_RHP.pdf', 'Garuda Construction and Engineering_RHP.pdf', 'Godavari Biorefineries_RHP.pdf', 'HVAX Technologies_RHP.pdf', 'Identical Brains Studios Limited_RHP.pdf', 'Indian Phosphate_RHP.pdf', 'Innomet Advanced Materials_RHP.pdf', 'International Gemmological Institute (India) Limited_RHP.pdf', 'Jay Bee Laminations_RHP.pdf', 'Jeyyam Global Foods_RHP.pdf', 'Jungle Camps India_RHP.pdf', 'KRN Heat Exchanger and Refrigeration_RHP.pdf', 'Kross Limited_RHP.pdf', 'Lakshya Powertech_RHP.pdf', 'Lamosaic India_RHP.pdf', 'Mach Conferences and Events_RHP.pdf', 'Mangal Compusolution_RHP.pdf', 'My Mudra Fincorp_RHP.pdf', 'NTPC Green Energy_RHP.pdf', 'Namo eWaste Management_RHP.pdf', 'Naturewings Holidays_RHP.pdf', 'Neelam Linens and Garments_RHP.pdf', 'Neopolitan Pizza and Foods_RHP.pdf', 'NewMalayalam Steel Limited_RHP.pdf', 'Nexxus Petro Industries_RHP.pdf', 'Nisus Finance Services Co_RHP.pdf', 'Northern Arc Capital_RHP.pdf', 'OBSC Perfection_RHP.pdf', 'Onyx Biotec_RHP.pdf', 'Osel Devices_RHP.pdf', 'P N Gadgil Jewellers_RHP.pdf', 'Paramatrix Technologies_RHP.pdf', 'Paramount Dye Tec_RHP.pdf', 'Paramount Speciality Forgings_RHP.pdf', 'Popular Foundations_RHP.pdf', 'Pranik Logistics_RHP.pdf', 'Premier Energies_RHP.pdf', 'Premium Plast_RHP.pdf', 'Purple United Sales Limited_RHP.pdf', 'Rajesh Power Services_RHP.pdf', 'Rapid Multimodal Logistics_RHP.pdf', 'Resourceful Automobile_RHP.pdf', 'SPP Polymers_RHP.pdf', 'Sahasra Electronics Solutions_RHP.pdf', 'Saj Hotels_RHP.pdf', 'Share Samadhan_RHP.pdf', 'Shree Tirupati Balajee Agro Trading Company_RHP.pdf', 'Shubhshree Biofuels Energy_RHP.pdf', 'Sodhani Academy of Fintech Enablers_RHP.pdf', 'Subam Papers_RHP.pdf', 'Supreme Facility Management Limited_RHP.pdf', 'Swiggy_RHP.pdf', 'Thinking Hats Entertainment Solutions_RHP.pdf', 'Tolins Tyres_RHP.pdf', 'Toss The Coin_RHP.pdf', 'Trafiksol ITS Technologies_RHP.pdf', 'Travels & Rentals_RHP.pdf', 'Unilex Colours and Chemicals_RHP.pdf', 'United Heat Transfer_RHP.pdf', 'Usha Financial Services_RHP.pdf', 'Vdeal System_RHP.pdf', 'Ventive Hospitality Limited_RHP.pdf', 'Vision Infra Equipment Solutions_RHP.pdf', 'Waaree Energies_RHP.pdf', 'Western Carriers (India)_RHP.pdf', 'Yash High Voltage_RHP.pdf']\n",
      "Connecting to existing Pinecone index 'rag-chatbot-index-1'...\n",
      "Connecting to existing Pinecone index 'rag-chatbot-index-2'...\n",
      "Connecting to existing Pinecone index 'rag-chatbot-index-3'...\n",
      "Connecting to existing Pinecone index 'rag-chatbot-index-4'...\n",
      "Connecting to existing Pinecone index 'rag-chatbot-index-5'...\n",
      "\n",
      "Starting processing of 91 PDFs...\n",
      "\n",
      "[1/91] Processing PDF: Abha Power and Steel_RHP\n",
      "Extracting text from Abha Power and Steel_RHP...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking text from Abha Power and Steel_RHP...\n",
      "Generated 3990 chunks for Abha Power and Steel_RHP\n",
      "\n",
      "Processing Abha Power and Steel_RHP\n",
      "Total chunks to process: 3990\n",
      "Abha Power and Steel_RHP - Progress: 16.1% (643/3990 chunks)"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 286\u001b[0m\n\u001b[1;32m    283\u001b[0m chatbot \u001b[38;5;241m=\u001b[39m RAGChatbot(base_index_name\u001b[38;5;241m=\u001b[39mPINECONE_BASE_INDEX_NAME, num_indexes\u001b[38;5;241m=\u001b[39mNUM_PINECONE_INDEXES)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;66;03m# Ingest the PDFs\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m \u001b[43mchatbot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mingest_pdfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_paths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# Example Query\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[4], line 241\u001b[0m, in \u001b[0;36mRAGChatbot.ingest_pdfs\u001b[0;34m(self, pdf_paths)\u001b[0m\n\u001b[1;32m    238\u001b[0m chunks \u001b[38;5;241m=\u001b[39m semantic_chunking(extracted_text)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m chunks for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdf_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 241\u001b[0m summarized_chunks \u001b[38;5;241m=\u001b[39m \u001b[43msummarize_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpdf_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbedding and upserting chunks for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdf_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    244\u001b[0m index_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_index_for_document(pdf_name)\n",
      "Cell \u001b[0;32mIn[4], line 142\u001b[0m, in \u001b[0;36msummarize_chunks\u001b[0;34m(chunks, prompt, pdf_name, model_name)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mpdf_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Progress: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprogress\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_chunks\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m chunks)\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# Add a small delay to avoid rate limiting\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mError summarizing chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdf_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import google.generativeai as genai\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from pdf2image import convert_from_path\n",
    "from pinecone import Pinecone\n",
    "from PyPDF2 import PdfReader\n",
    "from together import Together\n",
    "\n",
    "# --- Configuration ---\n",
    "TOGETHER_AI_API_KEY = \"64880c44ef37384040dc253c954ed2f190c0e4702c3e80745e5eb78221f47376\"\n",
    "PINECONE_API_KEY = \"pcsk_4Y7Kbx_UDYP7pqs92wU4PkmYnAPzGdD6HUCmZnr5npQEdz18se3FwMhyUTcuRggekeTvFm\"\n",
    "PINECONE_ENVIRONMENT = \"us-east-1\"\n",
    "PINECONE_BASE_INDEX_NAME = \"rag-chatbot-index\"\n",
    "NUM_PINECONE_INDEXES = 5\n",
    "GOOGLE_API_KEY = \"AIzaSyBe7hdWbsCf6kQmyoMAUXbOlr7p8v1Tjhk\"\n",
    "EMBEDDING_MODEL_NAME = \"all-mpnet-base-v2\"\n",
    "LLM_MODEL_NAME = \"meta-llama/Llama-3-8b-chat-hf\"\n",
    "CHUNK_SIZE = 300\n",
    "CHUNK_OVERLAP = 50\n",
    "MAX_SUMMARY_TOKENS = 350\n",
    "MAX_RESPONSE_TOKENS = 512\n",
    "SUMMARY_TEMPERATURE = 0.4\n",
    "RESPONSE_TEMPERATURE = 0.4\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def load_prompt(prompt_file: Path) -> str:\n",
    "    \"\"\"Loads the prompt from the given text file.\"\"\"\n",
    "    try:\n",
    "        with open(prompt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Prompt file not found at {prompt_file}. Using default prompt.\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading prompt from {prompt_file}: {e}. Using default prompt.\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: Path) -> Dict[int, str]:\n",
    "    \"\"\"Extracts text from a PDF, trying PyPDF2 first, then Gemini Vision API.\"\"\"\n",
    "    try:\n",
    "        pdf_reader = PdfReader(str(pdf_path))\n",
    "        page_text = {}\n",
    "        for page_number in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_number]\n",
    "            text = page.extract_text()\n",
    "            page_text[page_number + 1] = text\n",
    "\n",
    "        total_words = sum(len(text.split()) for text in page_text.values())\n",
    "        if total_words >= 20:\n",
    "            return page_text\n",
    "        else:\n",
    "            print(f\"PyPDF2 extracted less than 20 words ({total_words}). Falling back to Gemini Vision API.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during PDF extraction with PyPDF2: {e}. Falling back to Gemini Vision API.\")\n",
    "\n",
    "    # Fallback to Gemini Vision API\n",
    "    try:\n",
    "        genai.configure(api_key=GOOGLE_API_KEY)\n",
    "        model = genai.GenerativeModel(model_name=\"gemini-1.5-pro\")\n",
    "        images = convert_from_path(pdf_path)\n",
    "        gemini_page_text = {}\n",
    "        pdf_name = os.path.splitext(os.path.basename(str(pdf_path)))[0]\n",
    "        output_dir = \"GeminiVisionResult\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        prompt_file_path = Path(\"prompt.txt\")\n",
    "        prompt = load_prompt(prompt_file_path)\n",
    "\n",
    "        if not images:\n",
    "            raise FileNotFoundError(f\"Could not convert the PDF to images\")\n",
    "\n",
    "        for i, img in enumerate(images):\n",
    "            page_number = i + 1\n",
    "            output_file_path = os.path.join(output_dir, f\"{pdf_name}_{page_number}.txt\")\n",
    "\n",
    "            try:\n",
    "                response = model.generate_content([prompt, img], generation_config={\"max_output_tokens\": 4096})\n",
    "                response.resolve()\n",
    "                gemini_page_text[page_number] = response.text\n",
    "                print(f\"Gemini processed page {page_number}\")\n",
    "            except Exception as page_err:\n",
    "                print(f\"Error processing page {page_number} with Gemini: {page_err}\")\n",
    "                gemini_page_text[page_number] = f\"Error: An error occurred during Gemini processing of page {page_number}: {page_err}\"\n",
    "        return gemini_page_text\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Could not find file: {e}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Gemini Vision API processing: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def semantic_chunking(text_dict: Dict[int, str], chunk_size: int = CHUNK_SIZE, chunk_overlap: int = CHUNK_OVERLAP) -> List[str]:\n",
    "    \"\"\"Chunks the extracted text semantically.\"\"\"\n",
    "    all_text = \"\\n\".join(text_dict.values())\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "        length_function=len,\n",
    "    )\n",
    "    return text_splitter.split_text(all_text)\n",
    "\n",
    "\n",
    "def summarize_chunks(chunks: List[str], prompt: str, pdf_name: str, model_name: str = LLM_MODEL_NAME) -> List[str]:\n",
    "    \"\"\"Summarizes a list of text chunks using Together AI's API with progress tracking.\"\"\"\n",
    "    if not TOGETHER_AI_API_KEY:\n",
    "        raise ValueError(\"TOGETHER_AI_API_KEY environment variable not set.\")\n",
    "\n",
    "    client = Together(api_key=TOGETHER_AI_API_KEY)\n",
    "    summarized_chunks = []\n",
    "    total_chunks = len(chunks)\n",
    "\n",
    "    print(f\"\\nProcessing {pdf_name}\")\n",
    "    print(f\"Total chunks to process: {total_chunks}\")\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        try:\n",
    "            prompt_with_chunk = f\"{prompt}\\n\\nChunk: {chunk}\"\n",
    "            response = client.chat.completions.create(\n",
    "                model=model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt_with_chunk}],\n",
    "                max_tokens=MAX_SUMMARY_TOKENS,\n",
    "                temperature=SUMMARY_TEMPERATURE,\n",
    "            )\n",
    "            summary = response.choices[0].message.content\n",
    "            summarized_chunks.append(summary)\n",
    "\n",
    "            # Progress update\n",
    "            progress = (i + 1) / total_chunks * 100\n",
    "            print(f\"\\r{pdf_name} - Progress: {progress:.1f}% ({i + 1}/{total_chunks} chunks)\", end=\"\", flush=True)\n",
    "\n",
    "            # Add a small delay to avoid rate limiting\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError summarizing chunk {i} of {pdf_name}: {e}\")\n",
    "            summarized_chunks.append(chunk)\n",
    "\n",
    "    print(f\"\\n✓ Completed processing {pdf_name}\\n\")\n",
    "    return summarized_chunks\n",
    "\n",
    "\n",
    "def embed_and_upsert_to_pinecone(chunks: List[str], index, document_name: str):\n",
    "    \"\"\"Embeds the text chunks and upserts them to Pinecone.\"\"\"\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
    "    batch_size = 32\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch_chunks = chunks[i: i + batch_size]\n",
    "        ids = [f\"chunk-{document_name}-{i}-{j}\" for j in range(len(batch_chunks))]\n",
    "        embeds = embeddings.embed_documents(batch_chunks)\n",
    "        metadata = [{\"text\": text, \"document\": document_name} for text in batch_chunks]\n",
    "        to_upsert = list(zip(ids, embeds, metadata))\n",
    "        index.upsert(vectors=to_upsert)\n",
    "    print(f\"Upserted {len(chunks)} chunks to Pinecone for document {document_name}.\")\n",
    "\n",
    "\n",
    "def generate_response(query: str, context: str, model_name: str = LLM_MODEL_NAME) -> str:\n",
    "    \"\"\"Generates a response using Together AI's API.\"\"\"\n",
    "    if not TOGETHER_AI_API_KEY:\n",
    "        raise ValueError(\"TOGETHER_AI_API_KEY environment variable not set.\")\n",
    "\n",
    "    client = Together(api_key=TOGETHER_AI_API_KEY)\n",
    "\n",
    "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=MAX_RESPONSE_TOKENS,\n",
    "            temperature=RESPONSE_TEMPERATURE,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return \"I apologize, but I encountered an error while generating the response. Please try again.\"\n",
    "\n",
    "\n",
    "def query_pinecone(query: str, index, top_k: int = 15) -> str:\n",
    "    \"\"\"Queries Pinecone for relevant chunks.\"\"\"\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
    "    query_vector = embeddings.embed_query(query)\n",
    "\n",
    "    results = index.query(vector=query_vector, top_k=top_k, include_values=False, include_metadata=True)\n",
    "    context = \"\\n\\n\".join([match.metadata[\"text\"] for match in results.matches if match.metadata[\"document\"] in query.lower()])\n",
    "    return context\n",
    "\n",
    "\n",
    "# --- RAG Chatbot Class ---\n",
    "class RAGChatbot:\n",
    "    def __init__(self, base_index_name: str, num_indexes: int):\n",
    "        if not PINECONE_API_KEY:\n",
    "            raise ValueError(\"Pinecone API key must be set.\")\n",
    "        self.pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "        self.base_index_name = base_index_name\n",
    "        self.num_indexes = num_indexes\n",
    "        self.indexes = {}\n",
    "        for i in range(1, num_indexes + 1):\n",
    "            index_name = f\"{self.base_index_name}-{i}\"\n",
    "            print(f\"Connecting to existing Pinecone index '{index_name}'...\")\n",
    "            try:\n",
    "                self.indexes[i] = self.pc.Index(index_name)\n",
    "            except Exception as e:\n",
    "                raise Exception(f\"Error connecting to Pinecone index '{index_name}': {e}\")\n",
    "\n",
    "    def _get_index_for_document(self, document_name: str) -> int:\n",
    "        \"\"\"Hashes the document name and distributes to an index.\"\"\"\n",
    "        hashed_name = int(hashlib.sha256(document_name.encode()).hexdigest(), 16)\n",
    "        return (hashed_name % self.num_indexes) + 1\n",
    "\n",
    "    def ingest_pdfs(self, pdf_paths: List[Path]):\n",
    "        \"\"\"Ingests a list of PDFs, chunks them, and uploads to Pinecone.\"\"\"\n",
    "        prompt_file_path = Path(\"prompt.txt\")\n",
    "        prompt = load_prompt(prompt_file_path)\n",
    "        \n",
    "        total_pdfs = len(pdf_paths)\n",
    "        print(f\"\\nStarting processing of {total_pdfs} PDFs...\")\n",
    "        \n",
    "        for pdf_index, pdf_path in enumerate(pdf_paths, 1):\n",
    "            pdf_name = os.path.splitext(os.path.basename(str(pdf_path)))[0]\n",
    "            print(f\"\\n[{pdf_index}/{total_pdfs}] Processing PDF: {pdf_name}\")\n",
    "            \n",
    "            try:\n",
    "                print(f\"Extracting text from {pdf_name}...\")\n",
    "                extracted_text = extract_text_from_pdf(pdf_path)\n",
    "                \n",
    "                if extracted_text:\n",
    "                    print(f\"Chunking text from {pdf_name}...\")\n",
    "                    chunks = semantic_chunking(extracted_text)\n",
    "                    print(f\"Generated {len(chunks)} chunks for {pdf_name}\")\n",
    "                    \n",
    "                    summarized_chunks = summarize_chunks(chunks, prompt, pdf_name)\n",
    "                    \n",
    "                    print(f\"Embedding and upserting chunks for {pdf_name}...\")\n",
    "                    index_id = self._get_index_for_document(pdf_name)\n",
    "                    embed_and_upsert_to_pinecone(summarized_chunks, self.indexes[index_id], pdf_name)\n",
    "                    \n",
    "                    print(f\"✓ Successfully completed processing {pdf_name}\")\n",
    "                    print(\"-\" * 80)\n",
    "                else:\n",
    "                    print(f\"⚠ No text extracted from PDF: {pdf_name}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error processing {pdf_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\n✓ Completed processing all {total_pdfs} PDFs\")\n",
    "\n",
    "    def query(self, query: str) -> str:\n",
    "        \"\"\"Queries the chatbot with a user's question.\"\"\"\n",
    "        index_id = self._get_index_for_document(query)\n",
    "        context = query_pinecone(query, self.indexes[index_id])\n",
    "        if not context:\n",
    "            return \"No relevant information found in the document.\"\n",
    "        return generate_response(query, context)\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Set your directory\n",
    "    pdfs_directory = Path(r\"/teamspace/studios/this_studio/hdfc-securities/Documents/RHP_Documents\")\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(pdfs_directory, exist_ok=True)\n",
    "\n",
    "    # Get all PDF files from the directory\n",
    "    pdf_paths = list(pdfs_directory.glob(\"*.pdf\"))\n",
    "    if not pdf_paths:\n",
    "        raise FileNotFoundError(f\"No PDF files found in the directory {pdfs_directory}. Ensure a folder named pdfs exist in the same directory\")\n",
    "    else:\n",
    "        print(f\"PDFs found in the directory: {[os.path.basename(str(file)) for file in pdf_paths]}\")\n",
    "\n",
    "    # Initialize the RAG chatbot with an existing Pinecone index name\n",
    "    chatbot = RAGChatbot(base_index_name=PINECONE_BASE_INDEX_NAME, num_indexes=NUM_PINECONE_INDEXES)\n",
    "\n",
    "    # Ingest the PDFs\n",
    "    chatbot.ingest_pdfs(pdf_paths)\n",
    "\n",
    "    # Example Query\n",
    "    while True:\n",
    "        user_query = input(\"Question: \")\n",
    "        start_time = time.time()\n",
    "        response = chatbot.query(user_query)\n",
    "        end_time = time.time()\n",
    "        print(f\"Answer: {response}\")\n",
    "        print(f\"Time taken: {end_time - start_time:.4f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
